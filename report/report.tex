
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
%\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
%\usepackage{sectsty}
%\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}						% Remove header underlines
\renewcommand{\footrulewidth}{0pt}						% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}			% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
	%\vspace{-1in} 	
	\usefont{OT1}{bch}{b}{n}
	\normalfont \normalsize \textsc{University of Central Arkansas} \\ [25pt]
	\horrule{0.5pt} \\[0.4cm]
	\large Independent Study Report CRN: 31231 \\
	\huge Path Towards Caffeinated Deep Learning \\
	\horrule{2pt} \\[0.5cm]
}
\author{
	\normalfont 			\normalsize
	Abay Bektursun\\[-3pt]	\normalsize
	Fall 2017\\[-3pt]	\normalsize
}
\date{}



%%% Begin document
\begin{document}
	
	\maketitle
	\section{Fundamentals of Deep Learning}
		In this section I will shed some light onto the mathematical and computational concepts needed for understanding the basics of Deep Learning or any analytical methods for that matter. Most of the mathematical notation utilized here is same or similar to the one used in the famous "Deep Learning Book" [TODO: .bib]	
	
	\subsection{Challenges in Numerical Computation}
		Due to the limited nature of computer memory it is hard to store accurate representations of long real numbers. Therefore, we often approximate and round using floating points. This means that we will always have some errors when performing any useful numerical computations, even with higher precision floating points. Although there are numerical scientists who dedicate their careers to solving this kinds of problems, so we would not have to, it still important to be aware of the issues and challenges. 
	\paragraph{Overflow and Underflow.}
	There are cases where small rounding error can cause big problems in spite of the fact that approximations are often better than we even need them to be. For example, when a very small number is approximated to be zero. This problem is called Underflow. It's easy to imagine how things can go nuclear with Underflow: division by zero, or something along the lines of: $ \frac{1}{10^5}exp(95) $ will be zero if the fraction term is under approximated, but the true answer is about $ 10686.47458 $. Overflow is equivalently harmful problem. It occurs when a number with large absolute value is approximated to be $-\infty $ or $+\infty $. Consider the next: 
	\begin{align}
	softmax(x)_{i} = \frac{exp(x_{i})}{\sum_{j=1}^{n}exp(x_{j})}
	\end{align}
	This is Softmax function (1.1), it is often placed at the end of ANN topology. It is used to predict the probabilities associated with a multinoulli distribution. If all the values of $ x_{i} $ are equal to some arbitrarily large number, exponentiation will overflow, which in turn will lead to undefined answer. 
	\paragraph{Poor Conditioning}  
	The other common problem in numerical computing is poor conditioning. It refers to the problem of having large unexpected changes in the output of a function for small changes in the function's input. This kind of function with poor high condition is called ill-conditioned as oppose to well-conditioned problem.
	
	\subsection{Relevant Optimization Methods}
	\subsubsection{First-Order Optimization Algorithms}
	[gradient descent]
	\subsubsection{Second-order Optimization Algorithms}
	The optimization methods that involve second order derivatives are called Second-order. There are some advantages of Second-order Optimization Algorithms, but unfortunately they do not always work, if at all. The example of such algorithm would be one that uses Newton's method to jump the desired optima. 
	
	\paragraph{Different Gradient Descent Algorithms}
	
	\subsection{Neural Networks}
	
	
	\section{Introduction to Caffe}
	\section{Training Convolutional Neural Network on Caffe}
	\section{Testing and Tuning Hyperparameters}
	\section{Analysis of the Results and Further Investigation of Conv Net Architectures}
	\section{Classification of Skin Cancer Using GPU Accelerated [Architecture] on Caffe}
	\section{Analysis of the Skin Cancer Classification Results}
	
	
	\section{Lists}
	
	\subsection{Example for list (3*itemize)}
	\begin{itemize}
		\item First item in a list 
		\begin{itemize}
			\item First item in a list 
			\begin{itemize}
				\item First item in a list 
				\item Second item in a list 
			\end{itemize}
			\item Second item in a list 
		\end{itemize}
		\item Second item in a list 
	\end{itemize}
	
	\subsection{Example for list (enumerate)}
	\begin{enumerate}
		\item First item in a list 
		\item Second item in a list 
		\item Third item in a list
	\end{enumerate}
	%%% End document
	
	\begin{align} 
	\begin{split}
	(x+y)^3 	&= (x+y)^2(x+y)\\
	&=(x^2+2xy+y^2)(x+y)\\
	&=(x^3+2x^2y+xy^2) + (x^2y+2xy^2+y^3)\\
	&=x^3+3x^2y+3xy^2+y^3
	\end{split}					
	\end{align}
	
	\begin{align}
	A = 
	\begin{bmatrix}
	A_{11} & A_{21} \\
	A_{21} & A_{22}
	\end{bmatrix}
	\end{align}
	

\bibliographystyle{plain}
\bibliography{ref}


\end{document}
