
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
%\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}
\usepackage{listings}
\usepackage{lmodern}
\usepackage{accents}


%%% Custom sectioning
%\usepackage{sectsty}
%\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}						% Remove header underlines
\renewcommand{\footrulewidth}{0pt}						% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}			% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
	%\vspace{-1in} 	
	\usefont{OT1}{bch}{b}{n}
	\normalfont \normalsize \textsc{University of Central Arkansas} \\ [25pt]
	\horrule{0.5pt} \\[0.4cm]
	\large Independent Study Report CRN: 31231 \\
	\huge Path Towards Caffeinated Deep Learning \\
	\horrule{2pt} \\[0.5cm]
}
\author{
	\normalfont 			\normalsize
	Abay Bektursun\\[-3pt]	\normalsize
	Fall 2017\\[-3pt]	\normalsize
}
\date{}



%%% Begin document
\begin{document}
	
	\maketitle
	\section{Fundamentals of Deep Learning}
		In this section I will shed some light onto the mathematical and computational concepts needed for understanding the basics of Deep Learning or any analytical methods for that matter. Most of the mathematical notation utilized here is same or similar to the one used in the famous "Deep Learning Book" [TODO: .bib]	
	
	\subsection{Challenges in Numerical Computation}
		Due to the limited nature of computer memory it is hard to store accurate representations of long real numbers. Therefore, we often approximate and round using floating points. This means that we will always have some errors when performing any useful numerical computations, even with higher precision floating points. Although there are numerical scientists who dedicate their careers to solving this kinds of problems, so we would not have to, it still important to be aware of the issues and challenges. 
	\paragraph{Overflow and Underflow.}
	There are cases where small rounding error can cause big problems in spite of the fact that approximations are often better than we even need them to be. For example, when a very small number is approximated to be zero. This problem is called Underflow. It's easy to imagine how things can go nuclear with Underflow: division by zero, or something along the lines of: $ \frac{1}{10^5}exp(95) $ will be zero if the fraction term is under approximated, but the true answer is about $ 10686.47458 $. Overflow is equivalently harmful problem. It occurs when a number with large absolute value is approximated to be $-\infty $ or $+\infty $. Consider the next: 
	\begin{align}
	softmax(x)_{i} = \frac{exp(x_{i})}{\sum_{j=1}^{n}exp(x_{j})}
	\end{align}
	This is Softmax function (1.1), it is often placed at the end of the deep networks. It is used to predict the probabilities associated with a multinoulli distribution. If all the values of $ x_{i} $ are equal to some arbitrarily large number, exponentiation will overflow, which in turn will lead to undefined answer. 
	\paragraph{Poor Conditioning}  
	The other common problem in numerical computing is poor conditioning. It refers to the problem of having large unexpected changes in the output of a function for small changes in the function's input. This kind of function with poor high condition is called ill-conditioned as oppose to well-conditioned problem.
	
	\subsection{Relevant Optimization Methods}
    In the heart of Machine Learning and therefore Deep Learning there lie mathematical optimization algorithms. The mathematical concepts behind most of the optimization methods existed for long time, but we were able to harness their power only in the recent past. The driving force behind Deep Learning is optimization algorithm called Backpropagation, which is concrete implementation of Gradient Descent (reviewed in the next subsections), and an application of the chain rule, the rule that was around since Leibniz times. There are few reasons why it took so long to come up with modern machine learning approaches, the obvious one being non-existence of practical computational power for most of the scientific history. Yoshua Bengio also has given a great answer on Quora to a question "Why did it take so long to invent the backpropagation algorithm?" [https://goo.gl/MT9mIo]. As he says: "Lots of apparently obvious ideas only became obvious after the fact..." I believe that theoretical work of pure mathematics have great potential that will drive the future Deep Learning algorithms and help us put together the first blue print of true intelligence.\par
    As the name suggest the main objective of a Deep Learning system is to learn, which is done through optimization. To have a clear understanding of what we are optimizing for, it is appropriate to use Mitchell's [1997] definition of learning: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E." In supervised Deep Learning, performance measure P is the prediction accuracy of a system. Thus, our main objective is to maximize the P, concretely, to minimize the error of prediction. The main optimization approach used in Deep Learning is gradient based, and that is what we will be focusing on next.  
	\subsubsection{First-Order Optimization Algorithms}
    Given a function $ f(x) $, we need to minimize it by changing parameter $ x $ such that $ x* = arg\ min f(x)$. Superscript $*$ denotes a optimal value that optimizes $f(x)$. The function we are trying to optimize is called cost function, also known as objective function, loss function, criterion, and error function depending on literature and field. \par 
    First-Order optimization algorithm tries to minimize the cost function using its first order derivative $\frac{d}{dx}f(x)$ or shorter notation $f'(x)$. The main technique of doing so is Gradient Descent mentioned earlier. Derivative of a function tells us how the change in its input affects its output, and we could derive that for some input change $\epsilon $:\newline $f(x + \epsilon) \approx f(x) + \epsilon f'(x)$. It is also know that $ f(x - \epsilon sign(f'(x)) )  < f(x) $ given that $\epsilon$ is small enough. This is illustrated below (Fig 1.1). In the context of machine learning, $\epsilon$ is called learning rate, it is important hyperparameter for fine-tuning your learning system. \par
    
\begin{figure}[!htb]
	\centering
    \includegraphics[scale=0.6]{gradient_descent.png}
    \caption{The blue line $ f(x) = \frac{1}{2}x^2$ and the green line $f'(x) = x $ visualize how we can arrive at the global minima of a convex function by shifting $x$ in with small enough $\epsilon$ sized steps towards opposite sign of the derivative of $f(x)$}
    \label{fig:fx_dfdx}
\end{figure}

Some terminology from Calculus:
\begin{itemize}
		\item Critical Points also known as Stationary Points are the points where $ f'(x) = 0 $ 
		\item Local Minima and Local Maxima is a point at some interval of $x$ where $f(x)$ is the smallest or largest respectively.
        \item Global Minima and Global Maxima are the absolute minimum or maximum point of $f(x)$ in all of $f(x)'s$ domain
        \item Saddle Points are the critical points that are neither any kind of maxima nor minima
	\end{itemize}
    It is unlikely that you will ever have a single variable cost function. Most Deep Learning problems train on image, video, and audio data sets. It is not rear that input of your system will be a hundred thousand dimensional vector. Therefore, we need to generalize gradient descent to $f: \mathbb{R}^n \rightarrow \mathbb{R} $. Notice that the output of the function is still scalar, this needs to be true in order to minimization objective to make sense. \par
    Instead of using derivative $\frac{d}{dx}f(x)$, we will need to use partial derivative, for example: $\frac{\partial}{\partial x_{i} }f(x)$ that tells us how output of $f(x)$ changes with respect to $x_{i}$ at point $x$, where $x_{i}$ is one of the variables in the input vector $x$. Gradient is an equivalent to the derivative in high dimensions. We write it as $\nabla_{x} f(x)$. Gradient is a vector consisting of all partial derivatives of $f(x)$. To find a critical point of high dimensional $f(x)$ is to find the gradient $\nabla_{x} f(x)$ of which all values are equal to zero. We could visualize a gradient using vector field. Here is an example with $\mathbb{R}^2$ gradient (Fig. 1.2).\par

\begin{figure}[!htb]
	\centering
    \includegraphics[scale=0.6]{Quiver_edited.png}
    \caption{Illustration of 2D  vector field where $x$-axis corresponds to the $x_0$ variable  and $y$-axis corresponds to $x_1$ variable. Here, you can see the area along the upward diagonal stripe in the middle of the vector filed where the arrows are very short. This area corresponds to $ x $ vectors where gradient gets closer and closer to zero}
    \label{fig:Quiver_gradient}
\end{figure}

 Now, we can actually talk about the main topic of this subsection, the steepest descent. The directional derivative in direction of unit vector $\vec{u}$ is slope of $f$ in direction $\vec{u}$. Directional derivative is formally defined as $$ \frac{\partial}{\partial \vec{u}} f(\vec{x}) = \lim_{h\to0} \frac{f(\vec{x} + h\vec{u}) - f(\vec{x})}{h} = \nabla_{\vec{v}} f(\vec{x})$$
We could look at one intuitive algebraic definition. Let say that we have a 2D vector $\vec{u} = \begin{bmatrix} a \\ b \end{bmatrix}$. Then our directional derivative of $f(\vec{x})$ in the direction of $\vec{u}$ is $a\frac{\partial}{\partial x_0}f + b\frac{\partial}{\partial x_1}f$. Notice that this is same as $\begin{bmatrix} a \\ b \end{bmatrix} \boldsymbol{\cdot} \begin{bmatrix} \frac{\partial f}{\partial x_0} \\  \frac{\partial f}{\partial x_1} \end{bmatrix} $ The second vector is the gradient so we can rewrite this as $\vec{u} \boldsymbol{\cdot} \nabla_{\vec{v}} f(\vec{x}) = \vec{u}^T \nabla_{\vec{v}} f(\vec{x}) $ This what we would get if we derived from the formal definition using chain rule.  \par
So we would like to find direction in which $f$ decreases fastest, or we would like to minimize $f$. We do it making use of directional derivative 
$$\underset{u,u^Tu=1}{\text{min}} u^T\nabla_x f(x) =$$
$$ = \underset{u,u^Tu=1}{\text{min}} ||u||_2||\nabla_x f(x)||_2cos(\theta) $$


	\subsubsection{Second-order Optimization Algorithms}
	The optimization methods that involve second order derivatives are called Second-order. There are some advantages of Second-order Optimization Algorithms, but unfortunately they do not always work, if at all. The example of such algorithm would be one that uses Newton's method to jump the desired optima. 
	\subsection{Regularization}
	The difference between regular optimization and Deep Learning is that the latter does not optimize for the objective function directly. If a model was optimized to perfectly fit the training set, it will not be able to generalize well, and fail to achieve desired accuracy when exposed to new data. Regularization is a set of strategies that help us optimize our models while making them more general. Thus, regularization is one of the central themes in DL. There are many ways to perform regularization, among them: adding constraints to the parameters, adding soft constraints in form of additions of extra terms to the cost function, combining multiple hypothesis (ensemble method), and generating new data by adding noise to existing dataset.
	\par
	The simplest regularization strategy is to limit the capacity of a model by adding a parameter norm penalty $ \Omega(\theta) $.
	$$ 
	\hat{J}(\theta;X,y) = J(\theta,X,y) + \alpha\Omega(\theta) 
	$$ 
	As previously defined $J$ is the cost function, and $\hat{J}$ is a regularized cost function. $\alpha$ is a hyperparameter for fine tunning needed amount of regularization. Assigning $\alpha$ higher value will make generalization stronger. The problem when model performs well on a training set but not on a test set is called overfitting, giving $\alpha$ higher value can remedy the problem. However, if it is too high, we will introduce underfitting, meaning model has a low capacity and performs bad in all the datasets. When we use penalty regularization, we only regularize the model  parameters (weights or synapses) without biases (intercept terms). Regularizing the biases will hurt model badly, introducing huge underfitting problems. Regularization also works better when every layer has it's own version of hyperparameter $\alpha$. \par
	So, what is the $\Omega(\theta)$ function? The most common version of the function is $L^2$ parameter norm. It's called \textbf{weight decay}, also know as \textbf{ridge regression} or \textbf{Tikhonov Regularization}. It's defined to be:
	$$ 
	\Omega(\theta) = \frac{1}{2}||w||_2^2
	$$
	Notice that the input variable is $\theta$ but the function involves the $L^2$ norm of the $w$, here $w$ is a parameter vector without bias terms. Deriving from the original definition of the regularized cost function $\hat{(J)}$ we could arrive at this regularized parameter update with the gradient:
	$$
	w \leftarrow (1-\epsilon\alpha)w - \epsilon \nabla_w J(w;X,y).
	$$
	This is a precursor to the next section, but we could see that regularized version of the gradient update is modified to shrink the $w$ vector by constant factor on each step (step is single iteration of the backpropagation algorithm). 
	\par Let's look at closer analysis of weight decay with linear regression. The cost function:
	$$ 
	(Xw-y)^\top(Xw-y)
	$$
	We add $L^2$ regularization:
	$$
	(Xw-y)^\top(Xw-y) + \frac{1}{2} \alpha w^\top w
	$$
	The solution with normal equations gives us:
	$$
	w = (X^\top X + \alpha I)^{-1}X^\top y
	$$
	For contrast, this is the solution to non regularized cost:
	$$
	w = (X^\top X)^{-1}X^\top y
	$$
	The difference is that we replaced $(X^\top X)$ with $(X^\top X + \alpha I)$. The matrix $X^\top X$ is proportional to the covariance matrix $\frac{1}{m}X^\top X$. In the regularized version, we are adding $\alpha$ to the diagonal, this is same as adding $\alpha$ to the variance each input variable, therefore making learning system perceive the design matrix $X$ as having higher variance. This in turn causes the learner to shrink the parameters on features whose covariance with the output target is lower than the added variance.
	\par
	One of the main reasons why machine learning models are becoming ever so powerful is availability of large amounts of data. The rule of thumb is the more training data you have, the less regularization is needed. Unfortunately, the amount of data we could acquire is usually limited. This problem could be solved by generating new data using the existing training data. This technique is called \textbf{Data Augmentation}. It's done by adding random noise to the data [Sietsma and Dow, 1991], and rotating and flipping the input features. This works especially well with highly dimensional problems like computer vision, and audio processing. The noise can be added both directly to data and hidden layers [Poole at al., 2014]. Data augmentation is proven to work very well. One of the specific examples of augmentation is regularization method called \textbf{Dropout} which can be seen as generating new input by multiplying by noise. We will next look into ensemble methods which will lead us to understanding Dropout.
	\par 
	\textbf{Bagging} is a popular regularization technique and it stands for \textbf{bootstrap aggregation} [Breiman, 1994]. The idea behind bagging is training multiple models, and combining them to perform a common task, this also known as \textbf{model averaging}. Algorithms that use this strategy are called \textbf{ensemble methods}. Let's say we have $k$ regression models. Then the error associated with each model is $\epsilon_i$, and we assume that error drawn from zero mean multivariate normal distribution with variance $v = \mathbb E[\epsilon_i^2] $ and covariance $c = \mathbb E[\epsilon_i\epsilon_j] $. Then the average error is $\frac{1}{k}\sum_i \epsilon_i$. The expected squared error is:
	$$
	\mathbb E\bigg[\bigg(\frac{1}{k}\sum_i \epsilon_i\bigg)^2\bigg] = \frac{1}{k^2}\mathbb E\bigg[  \sum_i\bigg( \epsilon_i^2 + \sum_{j \not = i} \epsilon_i\epsilon_j \bigg) \bigg] = \frac{1}{k}v + \frac{k-1}{k}c
	$$
	If $c = v $, errors are perfectly correlated meaning model averaging is not going to be helpful. If $c=0$
	then the error are perfectly uncorrelated and the ensemble error equals $\frac{1}{k}v$, which tells that bigger the ensemble size, smaller the error, and smaller than error of any individual model of the ensemble.
	This idea of combing models is so powerful that making ensemble bigger almost linearly improves the performance of a Deep Learning system. All the latest competition winners employ large ensembles of networks like Netflix Grand Prize winner [Koren, 2009]. However, ensemble methods are not used when developing new Deep Learning methods since research teams with powerful hardware will be able to train much better models just by adding more networks, and making benchmarking the new algorithms very hard.

	\subsection{Neural Networks}
	Historically, neural networks were introduced in 1940s and were first developed as shallow linear models [cite]. Since first neural networks were linear, they could not even fit a simple functions like XOR (Which is discussed int he next subsection), and received lot of backlash which almost entirely killed the filed. Furthermore, critics of neural networks did not like the sentiment of people taking nature inspired approach. Non linear learning was achieved during 1960-1970s when computer scientists [Kelly, Denham, Bryson 1960;] were able to develop dynamic methods for computing gradient.
	The first push that brought Neural Networks to live was the book called Parallel Distributed Processing which showcased successful applications of learning though backpropagation.
	One of the first commercial applications of neural networks were developed by Dr. Yann LeCun who is the father of Convolutional Neural networks. In 1988, Dr. LeCun started working as a research scientists for Bell Labs where he developed the most successful OCR (Optical character recognition) of its time using his multilayer neural network architecture. He created a dataset that allowed other developers and scientists to develop their own OCR systems. It's called MNIST and it is currently the most famous problem to solve in the introductory neural networks courses. I had a pleasure to  have a brief interaction with Dr. LeCun on Facebook (Figure 1.3):
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.55]{sempai.PNG}
		\caption{Sempai}
		\label{fig:sempai}
	\end{figure}
	\newline After industry had the taste of neural networks in 1990s, people started realizing the unreasonable effectiveness of the technology that can be developed using neural networks. Slowly but surely neural networks were gaining popularity, and by 2000s the field was on the of the fast moving fields in computer science. Companies like Google started achieving amazing results, which sparked interest in many developers and scientists. New competitions and challenges like ImageNet were being held and they gave birth to a new field in around 2006 - Deep Learning. Today, Deep Learning has it's own culture that is separate from all the other machine learning and statistics communities. There are already dozens of successful deep learning architectures, but the most common one is Deep Feedforward Network which in essence is multilayer perceptron neural network. We will only focus on Feedforward Networks since they are most useful and could be put together using any Machine Learning framework like Caffe (Feedforward Nets even have SAS implementations).
	\par 
		The objective of Deep Forward network is to get as close approximation of function $f^*$ as possible. Where $f^*$ is true a function which maps input data $x$ to a class $y$ such that $ f^*(x) = y $. For example $f^*$ can be a function that maps image of a digit to vector $y$ where one of the values of $y^j = 1$ corresponds to the correct class $j$ to which the digit on image belongs to.
	Feed Forward networks have a good self explanatory name. Data is consumed in feed forward chain manner, where the first layer of the network is $f_1(x)$, the second layer is $f_2(x)$ and so on until $f_n(x)$. The data $x$ is fed to the firs layer, and then its output will be the input of the next layer, and its out to the next. The entire network will form a function $f(x) = f_n(..(f_3(f_2(f_1(x)))..) $ which is hopefully $\approx f^*(x)$. Deep Learning earned its name because of it's deep architecture with many layers. A networks with $n$ layers is said to have depth of $n$. The final layer is called output layer which is in case of Feedforward network is a vector of classes, or probabilities of data belonging to a particular class. The layers between the input and output layers are called hidden layers, and they are the main ingredient of Deep Learning that makes it the most powerful learning technique. Deep Learning extends the idea if linear models form regression techniques, but instead of approximating $f(x)$ directly, it applies the linear model to non linearly transformed space $\phi(x)$. We can think of $\phi$ transformation as giving a new representation of the data. I subjectively, find this non linear transform by hidden layers to be most beautiful thing about Deep Learning. To have a more complete picture Deep Feedforward network we write our model to be $y = \phi(x;\theta)^\top w $. Where $\phi$ learns parameters $\theta$ from the data $x$. $w$ are the weights that map $\phi$ to output layer.
	Unfortunately, it turns out that Deep Feedforward networks, unlike manually engineered features or radial basis kernels, give up convexity of training problems. However, the benefits of deep network overweight all the challenges. A well trained network can automatically learn needed features and have a state of the art accuracy.
	
	
	\subsubsection{Feedforward Example}
	It's a good idea to walk thorough example of a feed forward network. The most common example shown in introductory Deep Learning courses is learning XOR (exclusive 'or') function. XOR takes in two binary values and returns 1 if only one of the two variables is equal to 1, returns 0 otherwise.
	Let $X$ be our design matrix that consists of all the possible input vectors. Let $y$ be the corresponding outputs. 
		$$ X = 
		\begin{bmatrix}
			0 & 0 \\
			0 & 1 \\
			1 & 0 \\
			1 & 1
		\end{bmatrix} 
		y = 
		\begin{bmatrix}
		0 \\
		1 \\
		1 \\
		0 
		\end{bmatrix}$$
	We set our cost function to me a mean squared error $J(\theta)$ just for this example. The Deep Learning book describes more appropriate functions for binary input [cite], but for simplicity sake using MSE would be appropriate:
	
		$$ J(\theta)=\frac{1}{4}\sum_{x\in X}^{}(y-f(x;\theta))^2 $$
	If we chose the form of the model to be linear $ f(x;w,b)=x^\top w + b $, cost can be minimized in closed form using normal equations [dlb]. People have already calculated this, and as a result obtained $ w = 0 $ and $ b = \frac{1}{2}$. However this linear model would have only one output 0.5 for any input, meaning it is not able to approximate XOR function. To solve this we introduce non linearity by adding a hidden layer. Let's call the hidden layer vector $h$ which is defined to be $ h = f_1(x,W,c) $ then output will be $ y = f_2(h;w,b)$ Where matrix $W$ and vector $w$ are the synapses (or weights if you do not like the sentiment), $c$ and $b$ are the intercept terms or biases. Putting functions together we construct a chained network:
		$$ f_2(f_1(x,W,c);w,b) $$
	
	
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.6]{xor_ex.png}
		\caption{XOR example, here the hidden layer is vector $ h = f_1 $}
		\label{fig:cifar}
	\end{figure}
	So, where does the non linearity come from? It comes from the activation in the hidden layer.
	We first apply an affine transformation on the input and pass it to the activation function $ f_1 = h = g(W^\top x + c)$ where $g$ is the activation function. The recommended activation function is the rectified linear unit or ReLU for short [Hinton 2010]. ReLU is simply $ g(z) = max\{0,z\} $
	Again, let's put the pieces together to from a complete network:
	$$ f(x; W,c,w,b) = w^\top max\{0,W^\top x + c\} + b $$
	
	After network learns the XOR function we can show that:
	$$ W = 
	\begin{bmatrix}
	1 & 1 \\
	1 & 1
	\end{bmatrix} 
	c = 
	\begin{bmatrix}
	0 \\
	-1 \\ 
	\end{bmatrix}
	w = 
	\begin{bmatrix}
	1 \\
	-2 \\ 
	\end{bmatrix} b = 0$$
	We can now feedforward through the network. First we start from transformation:
		$$ XW = 
		\begin{bmatrix}
		0 & 0 \\
		1 & 1 \\
		1 & 1 \\
		2 & 2
		\end{bmatrix} $$
	and we complete out first affine transformation by adding the intercept vector $c$:
		$$ 
		\begin{bmatrix}
		0 & -1 \\
		1 & 0 \\
		1 & 0 \\
		2 & 1
		\end{bmatrix} $$
	Now, we need to use activation function. If you have not noticed, in this space all the examples lie on a single line and linear model can not work with this to produce valid XOR.
	After we apply ReLU, our matrix looks like this:
	$$ 
	\begin{bmatrix}
	0 & 0 \\
	1 & 0 \\
	1 & 0 \\
	2 & 1
	\end{bmatrix} $$
	ReLU has changed the space, and the vectors no longer lie on a common line. Linear model now can predict the classes correctly. We apply linear transformation to get:
		$$ 
	\begin{bmatrix}
	0 \\
	1 \\
	1 \\
	0 
	\end{bmatrix} $$
	which is exactly the $y$ vector from the XOR definition. We did not apply bias $b$ since it equals to 0. In this idealized case network achieved absolute minimum of cost. In real useful example however, absolute is never reached and good approximation of $f^*$ is learned.
	
	\subsubsection{Cost function, Activation, and Layers} 
	Modern deep learning architectures employ the idea of learning conditional distribution with maximum likelihood. Cost function, also known as loss, is defined to be negative log likelihood. This method is called cross-entropy:
	$$ J(\theta) = -\mathbb E_{x,y \sim \hat{p}_{data}}log(p_{model}(y|x))$$
	Maximum likelihood approach of deriving the cost function is advantageous because it allows you not to worry about choosing the right function for every model. Instead, cost function is automatically determined by specifying a model $p(y|x)$.
	
	
	In the past, the most widely accepted activation function was logistic sigmoid:
	 $$ g(z) = \sigma(z) = \frac{1}{1 + e^{-z}}$$
	However, Yann LeCun in his famous paper "Efficient Backprop" showed that traditional sigmoid should not be used and recommended using hyperbolic tangent, also an sigmoidal unit:
	$$ g(z) = tanh(z) $$
	which then became popular. Hyperbolic tangent is closely related to the logistic sigmoid, since $tanh = 2\sigma (2z)-1$ basically, $ tanh $ is scaled version of sigmoid. For most of the Deep Learning problems both of these activations perform worse compare to piecewise ReLU. Sigmoidal activations saturate bad across most of their domain. When input $z$ is large positive value, sigmoids saturate to a high value, and when $z$ is very negative, they saturate to a low value. They are only useful when $z$ is near 0. This can lead gradient based learning to bad results. Thus, using sigmoids with deep feedforward networks is discouraged. However, architectures like recurrent neural networks and autoencoders have restriction due to which piece wise functions can not be used, and sigmoids are applied regardless of the saturation issues. There are several generalizations of ReLU, and one of them Maxuot unit. Maxout is not an element wise function like ReLU, instead it divides input into groups of $k$ values. 
		$$ g(z)_i = \underaccent{j \in \mathbb G^{(i)}}{max} \ z_j$$
	Maxout can generalize well with without regularization given that training set is large and the number of pieces per unit is low [Cai et at.,2013]
	\par Feedforward network can be thought of as a universal non linear function approximatior. The Universal Approximation Theorem (Hornik 1989) tells us that a feedforward network with at least one hidden layer with activation function can approximate any Borel measurable function from one finite dimensional space to another with any desired non zero error.
	
	\section{Introduction to Caffe}
	Caffe is a one of the few industry strength Deep Learning Frameworks. It is an open-source software created by The Berkeley Artificial Intelligence Research (BAIR) Lab that started out as Yangqing Jia's thesis who was UC Berkeley's PhD student at that time [http://daggerfs.com/]. The strength of Caffe is its performance and ease of use. According to its developers it was made with expression, speed, and modularity in mind. It's main weakness is flexibility, or absence of it. Architecture in Caffe is defined in protobuff format using predefined nodes and optimization algorithms, leaving the developer to arrange the nodes and tune hyperparameters. Caffe has been most widely used Deep Learning framework both in Academia and Industry, although that title has slipped to TensorFlow's ownership in the recent past. Even the original developer Yangqing Jia moved on to help developing TensorFlow. Seeing the rise of projects like TensorFlow, Torch, and Theano, it seems like Caffe will not be one of the leading frameworks of the future. Nevertheless, Caffe will have its place a great prototyping tool and solution for time constrained agile projects. Caffe has it's own philosophy [http://caffe.berkeleyvision.org]:
		\begin{itemize}
			\item Expression: models and optimizations are defined as plaintext schemas instead of code.
			\item Speed: for research and industry alike speed is crucial for state-of-the-art models and massive data. 
			\item Modularity: new tasks and settings require flexibility and extension.
			\item Openness: scientific and applied progress call for common code, reference models, and reproducibility.
			\item Community: academic research, startup prototypes, and industrial applications all share strength by joint discussion and development in a BSD-2 project.
		\end{itemize}
	
	\section{Training Convolutional Neural Network on Caffe}
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.4]{cifar.png}
		\caption{Visualization of the Caffe architecture used to train the Cifar models}
		\label{fig:cifar}
	\end{figure}

	There first 'data' layer is defined by a data node that takes in mdb data input (Caffe understandable data format) with shape (channels and size):
	\begin{lstlisting}[language=xml]
	{ dim: 1 dim: 3 dim: 32 dim: 32 }
	\end{lstlisting}
	By froward pass path, data layer is fed to 'conv1' which is defined as five filters with stride of one.
	
	
	\section{Testing and Tuning Hyperparameters}
	Here, model was trained on CIFAR-10 dataset that is consists of 60000 image with 3 color (RGB) channels and 32x32 pixels. The whole dataset is around 160Mb and has 10 independent classes with 6000 images for each. It is split into training and testing sets with 50000 and 10000 images in each respectively. 
	
	The highest score obtained in this example of course are not as good as state of the art object recognition results, since the training images are not always fair:
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=1.0]{cifar10.png}
		\caption{There are many images that do not carry information needed due to low resolution. Often predicting the class of an object is even hard for humans}
		\label{fig:cifar10ex}
	\end{figure}
	\newline Here is are descriptions of the system used for training: 
	\begin{verbatim}
	
	H/W path                Device      Class       Description
	=============================================================================
	system      To Be Filled By O.E.M. (To Be Filled By O.E.M.)
	/0/4/5                              memory      288KiB L1 cache
	/0/4/6                              memory      6MiB L2 cache
	/0/4/7                              memory      8MiB L3 cache
	/0/e                                memory      8GiB System Memory
	/0/100/2/0                          display     GM206 [GeForce GTX 960]
	\end{verbatim}
	Full:      
		61m 58.334s
	Test net output: accuracy = 0.8148
	\newline Quick: 
		0m 50.972s
	Test net output: accuracy = 0.7535
	

\bibliographystyle{plain}
\bibliography{ref}


\end{document}
