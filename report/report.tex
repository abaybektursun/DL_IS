
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
%\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
%\usepackage{sectsty}
%\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}						% Remove header underlines
\renewcommand{\footrulewidth}{0pt}						% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}			% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
	%\vspace{-1in} 	
	\usefont{OT1}{bch}{b}{n}
	\normalfont \normalsize \textsc{University of Central Arkansas} \\ [25pt]
	\horrule{0.5pt} \\[0.4cm]
	\large Independent Study Report CRN: 31231 \\
	\huge Path Towards Caffeinated Deep Learning \\
	\horrule{2pt} \\[0.5cm]
}
\author{
	\normalfont 			\normalsize
	Abay Bektursun\\[-3pt]	\normalsize
	Fall 2017\\[-3pt]	\normalsize
}
\date{}



%%% Begin document
\begin{document}
	
	\maketitle
	\section{Fundamentals of Deep Learning}
		In this section I will shed some light onto the mathematical and computational concepts needed for understanding the basics of Deep Learning or any analytical methods for that matter. Most of the mathematical notation utilized here is same or similar to the one used in the famous "Deep Learning Book" [TODO: .bib]\par
	Test 
	
	
	\subsection{Challenges in Numerical Computation}
		Due to the limited nature of computer memory it is hard to store accurate representations of long real numbers. Therefore, we often approximate and round using floating points. This means that we will always have some errors when performing any useful numerical computations, even with higher precision floating points. Although there are numerical scientists who dedicate their careers to solving this kinds of problems, so we would not have to, it still important to be aware of the issues and challenges. 
	\paragraph{Overflow and Underflow.}
	There are cases where small rounding error can cause big problems in spite of the fact that approximations are often better than we even need them to be. For example, when a very small number is approximated to be zero. This problem is called Underflow. It's easy to imagine how things can go nuclear with Underflow: division by zero, or something along the lines of: $ \frac{1}{10^5}exp(95) $ will be zero if the fraction term is under approximated, but the true answer is about $ 10686.47458 $. Overflow is equivalently harmful problem. It occurs when a number with large absolute value is approximated to be $-\infty $ or $+\infty $. Consider the next: 
	\begin{align}
	softmax(x)_{i} = \frac{exp(x_{i})}{\sum_{j=1}^{n}exp(x_{j})}
	\end{align}
	This is Softmax function (1.1), it is often placed at the end of ANN topology. It is used to predict the probabilities associated with a multinoulli distribution. If all the values of $ x_{i} $ are equal to some arbitrarily large number, exponentiation will overflow, which in turn will lead to undefined answer. 
	\paragraph{Poor Conditioning} 
	
	\subsection{Relevant Optimization Methods}
	\subsubsection{First-Order Optimization Algorithms}
	\subsubsection{Second-order Optimization Algorithms}
	\paragraph{Different Gradient Descent Algorithms}
	
	\subsection{Neural Networks}
	Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. 
	
	\section{Introduction to Caffe}
	\section{Training Convolutional Neural Network on Caffe}
	\section{Testing and Tuning Hyperparameters}
	\section{Analysis of the Results and Further Investigation of Conv Net Architectures}
	\section{Classification of Skin Cancer Using GPU Accelerated [Architecture] on Caffe}
	\section{Analysis of the Skin Cancer Classification Results}
	
	
	\section{Lists}
	
	\subsection{Example for list (3*itemize)}
	\begin{itemize}
		\item First item in a list 
		\begin{itemize}
			\item First item in a list 
			\begin{itemize}
				\item First item in a list 
				\item Second item in a list 
			\end{itemize}
			\item Second item in a list 
		\end{itemize}
		\item Second item in a list 
	\end{itemize}
	
	\subsection{Example for list (enumerate)}
	\begin{enumerate}
		\item First item in a list 
		\item Second item in a list 
		\item Third item in a list
	\end{enumerate}
	%%% End document
	
	\begin{align} 
	\begin{split}
	(x+y)^3 	&= (x+y)^2(x+y)\\
	&=(x^2+2xy+y^2)(x+y)\\
	&=(x^3+2x^2y+xy^2) + (x^2y+2xy^2+y^3)\\
	&=x^3+3x^2y+3xy^2+y^3
	\end{split}					
	\end{align}
	
	\begin{align}
	A = 
	\begin{bmatrix}
	A_{11} & A_{21} \\
	A_{21} & A_{22}
	\end{bmatrix}
	\end{align}
	

\bibliographystyle{plain}
\bibliography{ref}


\end{document}
