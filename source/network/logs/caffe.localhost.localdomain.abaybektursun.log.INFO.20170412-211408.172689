Log file created at: 2017/04/12 21:14:08
Running on machine: localhost.localdomain
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0412 21:14:08.669736 172689 caffe.cpp:185] Using GPUs 0
I0412 21:14:08.691334 172689 caffe.cpp:190] GPU 0: GeForce GTX 960
I0412 21:14:09.183136 172689 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.0001
display: 100
max_iter: 5000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/cifar10_quick"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_quick_train_test.prototxt"
snapshot_format: HDF5
I0412 21:14:09.183504 172689 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0412 21:14:09.184198 172689 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0412 21:14:09.184242 172689 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0412 21:14:09.184432 172689 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_quick"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0412 21:14:09.186100 172689 layer_factory.hpp:77] Creating layer cifar
I0412 21:14:09.186879 172689 net.cpp:91] Creating Layer cifar
I0412 21:14:09.186924 172689 net.cpp:399] cifar -> data
I0412 21:14:09.186980 172689 net.cpp:399] cifar -> label
I0412 21:14:09.187022 172689 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0412 21:14:09.188285 172740 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0412 21:14:09.202424 172689 data_layer.cpp:41] output data size: 100,3,32,32
I0412 21:14:09.208431 172689 net.cpp:141] Setting up cifar
I0412 21:14:09.208485 172689 net.cpp:148] Top shape: 100 3 32 32 (307200)
I0412 21:14:09.208508 172689 net.cpp:148] Top shape: 100 (100)
I0412 21:14:09.208528 172689 net.cpp:156] Memory required for data: 1229200
I0412 21:14:09.208556 172689 layer_factory.hpp:77] Creating layer conv1
I0412 21:14:09.208600 172689 net.cpp:91] Creating Layer conv1
I0412 21:14:09.208618 172689 net.cpp:425] conv1 <- data
I0412 21:14:09.208653 172689 net.cpp:399] conv1 -> conv1
I0412 21:14:09.500457 172689 net.cpp:141] Setting up conv1
I0412 21:14:09.500509 172689 net.cpp:148] Top shape: 100 32 32 32 (3276800)
I0412 21:14:09.500524 172689 net.cpp:156] Memory required for data: 14336400
I0412 21:14:09.500563 172689 layer_factory.hpp:77] Creating layer pool1
I0412 21:14:09.500604 172689 net.cpp:91] Creating Layer pool1
I0412 21:14:09.500623 172689 net.cpp:425] pool1 <- conv1
I0412 21:14:09.500648 172689 net.cpp:399] pool1 -> pool1
I0412 21:14:09.500732 172689 net.cpp:141] Setting up pool1
I0412 21:14:09.500752 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.500766 172689 net.cpp:156] Memory required for data: 17613200
I0412 21:14:09.500797 172689 layer_factory.hpp:77] Creating layer relu1
I0412 21:14:09.500823 172689 net.cpp:91] Creating Layer relu1
I0412 21:14:09.500838 172689 net.cpp:425] relu1 <- pool1
I0412 21:14:09.500857 172689 net.cpp:386] relu1 -> pool1 (in-place)
I0412 21:14:09.501158 172689 net.cpp:141] Setting up relu1
I0412 21:14:09.501188 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.501206 172689 net.cpp:156] Memory required for data: 20890000
I0412 21:14:09.501224 172689 layer_factory.hpp:77] Creating layer conv2
I0412 21:14:09.501256 172689 net.cpp:91] Creating Layer conv2
I0412 21:14:09.501277 172689 net.cpp:425] conv2 <- pool1
I0412 21:14:09.501302 172689 net.cpp:399] conv2 -> conv2
I0412 21:14:09.505533 172689 net.cpp:141] Setting up conv2
I0412 21:14:09.505578 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.505589 172689 net.cpp:156] Memory required for data: 24166800
I0412 21:14:09.505615 172689 layer_factory.hpp:77] Creating layer relu2
I0412 21:14:09.505632 172689 net.cpp:91] Creating Layer relu2
I0412 21:14:09.505645 172689 net.cpp:425] relu2 <- conv2
I0412 21:14:09.505661 172689 net.cpp:386] relu2 -> conv2 (in-place)
I0412 21:14:09.506058 172689 net.cpp:141] Setting up relu2
I0412 21:14:09.506080 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.506095 172689 net.cpp:156] Memory required for data: 27443600
I0412 21:14:09.506109 172689 layer_factory.hpp:77] Creating layer pool2
I0412 21:14:09.506125 172689 net.cpp:91] Creating Layer pool2
I0412 21:14:09.506137 172689 net.cpp:425] pool2 <- conv2
I0412 21:14:09.506153 172689 net.cpp:399] pool2 -> pool2
I0412 21:14:09.506386 172689 net.cpp:141] Setting up pool2
I0412 21:14:09.506402 172689 net.cpp:148] Top shape: 100 32 8 8 (204800)
I0412 21:14:09.506414 172689 net.cpp:156] Memory required for data: 28262800
I0412 21:14:09.506428 172689 layer_factory.hpp:77] Creating layer conv3
I0412 21:14:09.506448 172689 net.cpp:91] Creating Layer conv3
I0412 21:14:09.506459 172689 net.cpp:425] conv3 <- pool2
I0412 21:14:09.506475 172689 net.cpp:399] conv3 -> conv3
I0412 21:14:09.510063 172689 net.cpp:141] Setting up conv3
I0412 21:14:09.510087 172689 net.cpp:148] Top shape: 100 64 8 8 (409600)
I0412 21:14:09.510104 172689 net.cpp:156] Memory required for data: 29901200
I0412 21:14:09.510123 172689 layer_factory.hpp:77] Creating layer relu3
I0412 21:14:09.510149 172689 net.cpp:91] Creating Layer relu3
I0412 21:14:09.510165 172689 net.cpp:425] relu3 <- conv3
I0412 21:14:09.510188 172689 net.cpp:386] relu3 -> conv3 (in-place)
I0412 21:14:09.510426 172689 net.cpp:141] Setting up relu3
I0412 21:14:09.510448 172689 net.cpp:148] Top shape: 100 64 8 8 (409600)
I0412 21:14:09.510462 172689 net.cpp:156] Memory required for data: 31539600
I0412 21:14:09.510478 172689 layer_factory.hpp:77] Creating layer pool3
I0412 21:14:09.510524 172689 net.cpp:91] Creating Layer pool3
I0412 21:14:09.510540 172689 net.cpp:425] pool3 <- conv3
I0412 21:14:09.510555 172689 net.cpp:399] pool3 -> pool3
I0412 21:14:09.511023 172689 net.cpp:141] Setting up pool3
I0412 21:14:09.511045 172689 net.cpp:148] Top shape: 100 64 4 4 (102400)
I0412 21:14:09.511059 172689 net.cpp:156] Memory required for data: 31949200
I0412 21:14:09.511072 172689 layer_factory.hpp:77] Creating layer ip1
I0412 21:14:09.511096 172689 net.cpp:91] Creating Layer ip1
I0412 21:14:09.511109 172689 net.cpp:425] ip1 <- pool3
I0412 21:14:09.511124 172689 net.cpp:399] ip1 -> ip1
I0412 21:14:09.514721 172689 net.cpp:141] Setting up ip1
I0412 21:14:09.514760 172689 net.cpp:148] Top shape: 100 64 (6400)
I0412 21:14:09.514777 172689 net.cpp:156] Memory required for data: 31974800
I0412 21:14:09.514797 172689 layer_factory.hpp:77] Creating layer ip2
I0412 21:14:09.514825 172689 net.cpp:91] Creating Layer ip2
I0412 21:14:09.514842 172689 net.cpp:425] ip2 <- ip1
I0412 21:14:09.514866 172689 net.cpp:399] ip2 -> ip2
I0412 21:14:09.515096 172689 net.cpp:141] Setting up ip2
I0412 21:14:09.515117 172689 net.cpp:148] Top shape: 100 10 (1000)
I0412 21:14:09.515135 172689 net.cpp:156] Memory required for data: 31978800
I0412 21:14:09.515162 172689 layer_factory.hpp:77] Creating layer loss
I0412 21:14:09.515193 172689 net.cpp:91] Creating Layer loss
I0412 21:14:09.515210 172689 net.cpp:425] loss <- ip2
I0412 21:14:09.515228 172689 net.cpp:425] loss <- label
I0412 21:14:09.515244 172689 net.cpp:399] loss -> loss
I0412 21:14:09.515281 172689 layer_factory.hpp:77] Creating layer loss
I0412 21:14:09.515714 172689 net.cpp:141] Setting up loss
I0412 21:14:09.515738 172689 net.cpp:148] Top shape: (1)
I0412 21:14:09.515750 172689 net.cpp:151]     with loss weight 1
I0412 21:14:09.515796 172689 net.cpp:156] Memory required for data: 31978804
I0412 21:14:09.515810 172689 net.cpp:217] loss needs backward computation.
I0412 21:14:09.515826 172689 net.cpp:217] ip2 needs backward computation.
I0412 21:14:09.515842 172689 net.cpp:217] ip1 needs backward computation.
I0412 21:14:09.515857 172689 net.cpp:217] pool3 needs backward computation.
I0412 21:14:09.515871 172689 net.cpp:217] relu3 needs backward computation.
I0412 21:14:09.515887 172689 net.cpp:217] conv3 needs backward computation.
I0412 21:14:09.515904 172689 net.cpp:217] pool2 needs backward computation.
I0412 21:14:09.515920 172689 net.cpp:217] relu2 needs backward computation.
I0412 21:14:09.515936 172689 net.cpp:217] conv2 needs backward computation.
I0412 21:14:09.515954 172689 net.cpp:217] relu1 needs backward computation.
I0412 21:14:09.515969 172689 net.cpp:217] pool1 needs backward computation.
I0412 21:14:09.515985 172689 net.cpp:217] conv1 needs backward computation.
I0412 21:14:09.516003 172689 net.cpp:219] cifar does not need backward computation.
I0412 21:14:09.516016 172689 net.cpp:261] This network produces output loss
I0412 21:14:09.516043 172689 net.cpp:274] Network initialization done.
I0412 21:14:09.516671 172689 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_quick_train_test.prototxt
I0412 21:14:09.516718 172689 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0412 21:14:09.516906 172689 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_quick"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.1
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0412 21:14:09.518805 172689 layer_factory.hpp:77] Creating layer cifar
I0412 21:14:09.518982 172689 net.cpp:91] Creating Layer cifar
I0412 21:14:09.519006 172689 net.cpp:399] cifar -> data
I0412 21:14:09.519035 172689 net.cpp:399] cifar -> label
I0412 21:14:09.519064 172689 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0412 21:14:09.521922 172761 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0412 21:14:09.522111 172689 data_layer.cpp:41] output data size: 100,3,32,32
I0412 21:14:09.526479 172689 net.cpp:141] Setting up cifar
I0412 21:14:09.526520 172689 net.cpp:148] Top shape: 100 3 32 32 (307200)
I0412 21:14:09.526535 172689 net.cpp:148] Top shape: 100 (100)
I0412 21:14:09.526549 172689 net.cpp:156] Memory required for data: 1229200
I0412 21:14:09.526567 172689 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0412 21:14:09.526597 172689 net.cpp:91] Creating Layer label_cifar_1_split
I0412 21:14:09.526614 172689 net.cpp:425] label_cifar_1_split <- label
I0412 21:14:09.526636 172689 net.cpp:399] label_cifar_1_split -> label_cifar_1_split_0
I0412 21:14:09.526664 172689 net.cpp:399] label_cifar_1_split -> label_cifar_1_split_1
I0412 21:14:09.526757 172689 net.cpp:141] Setting up label_cifar_1_split
I0412 21:14:09.526774 172689 net.cpp:148] Top shape: 100 (100)
I0412 21:14:09.526796 172689 net.cpp:148] Top shape: 100 (100)
I0412 21:14:09.526813 172689 net.cpp:156] Memory required for data: 1230000
I0412 21:14:09.526831 172689 layer_factory.hpp:77] Creating layer conv1
I0412 21:14:09.526865 172689 net.cpp:91] Creating Layer conv1
I0412 21:14:09.526885 172689 net.cpp:425] conv1 <- data
I0412 21:14:09.526911 172689 net.cpp:399] conv1 -> conv1
I0412 21:14:09.528869 172689 net.cpp:141] Setting up conv1
I0412 21:14:09.528901 172689 net.cpp:148] Top shape: 100 32 32 32 (3276800)
I0412 21:14:09.528923 172689 net.cpp:156] Memory required for data: 14337200
I0412 21:14:09.528950 172689 layer_factory.hpp:77] Creating layer pool1
I0412 21:14:09.528996 172689 net.cpp:91] Creating Layer pool1
I0412 21:14:09.529013 172689 net.cpp:425] pool1 <- conv1
I0412 21:14:09.529036 172689 net.cpp:399] pool1 -> pool1
I0412 21:14:09.529101 172689 net.cpp:141] Setting up pool1
I0412 21:14:09.529122 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.529139 172689 net.cpp:156] Memory required for data: 17614000
I0412 21:14:09.529157 172689 layer_factory.hpp:77] Creating layer relu1
I0412 21:14:09.529175 172689 net.cpp:91] Creating Layer relu1
I0412 21:14:09.529194 172689 net.cpp:425] relu1 <- pool1
I0412 21:14:09.529214 172689 net.cpp:386] relu1 -> pool1 (in-place)
I0412 21:14:09.529719 172689 net.cpp:141] Setting up relu1
I0412 21:14:09.529744 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.529758 172689 net.cpp:156] Memory required for data: 20890800
I0412 21:14:09.529775 172689 layer_factory.hpp:77] Creating layer conv2
I0412 21:14:09.529804 172689 net.cpp:91] Creating Layer conv2
I0412 21:14:09.529824 172689 net.cpp:425] conv2 <- pool1
I0412 21:14:09.529844 172689 net.cpp:399] conv2 -> conv2
I0412 21:14:09.533090 172689 net.cpp:141] Setting up conv2
I0412 21:14:09.533125 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.533146 172689 net.cpp:156] Memory required for data: 24167600
I0412 21:14:09.533170 172689 layer_factory.hpp:77] Creating layer relu2
I0412 21:14:09.533438 172689 net.cpp:91] Creating Layer relu2
I0412 21:14:09.533540 172689 net.cpp:425] relu2 <- conv2
I0412 21:14:09.533619 172689 net.cpp:386] relu2 -> conv2 (in-place)
I0412 21:14:09.533939 172689 net.cpp:141] Setting up relu2
I0412 21:14:09.534034 172689 net.cpp:148] Top shape: 100 32 16 16 (819200)
I0412 21:14:09.534111 172689 net.cpp:156] Memory required for data: 27444400
I0412 21:14:09.534198 172689 layer_factory.hpp:77] Creating layer pool2
I0412 21:14:09.534281 172689 net.cpp:91] Creating Layer pool2
I0412 21:14:09.534359 172689 net.cpp:425] pool2 <- conv2
I0412 21:14:09.534440 172689 net.cpp:399] pool2 -> pool2
I0412 21:14:09.535045 172689 net.cpp:141] Setting up pool2
I0412 21:14:09.535145 172689 net.cpp:148] Top shape: 100 32 8 8 (204800)
I0412 21:14:09.535231 172689 net.cpp:156] Memory required for data: 28263600
I0412 21:14:09.535310 172689 layer_factory.hpp:77] Creating layer conv3
I0412 21:14:09.535405 172689 net.cpp:91] Creating Layer conv3
I0412 21:14:09.535483 172689 net.cpp:425] conv3 <- pool2
I0412 21:14:09.535567 172689 net.cpp:399] conv3 -> conv3
I0412 21:14:09.540093 172689 net.cpp:141] Setting up conv3
I0412 21:14:09.540135 172689 net.cpp:148] Top shape: 100 64 8 8 (409600)
I0412 21:14:09.540151 172689 net.cpp:156] Memory required for data: 29902000
I0412 21:14:09.540179 172689 layer_factory.hpp:77] Creating layer relu3
I0412 21:14:09.540206 172689 net.cpp:91] Creating Layer relu3
I0412 21:14:09.540222 172689 net.cpp:425] relu3 <- conv3
I0412 21:14:09.540242 172689 net.cpp:386] relu3 -> conv3 (in-place)
I0412 21:14:09.540498 172689 net.cpp:141] Setting up relu3
I0412 21:14:09.540519 172689 net.cpp:148] Top shape: 100 64 8 8 (409600)
I0412 21:14:09.540532 172689 net.cpp:156] Memory required for data: 31540400
I0412 21:14:09.540549 172689 layer_factory.hpp:77] Creating layer pool3
I0412 21:14:09.540566 172689 net.cpp:91] Creating Layer pool3
I0412 21:14:09.540581 172689 net.cpp:425] pool3 <- conv3
I0412 21:14:09.540599 172689 net.cpp:399] pool3 -> pool3
I0412 21:14:09.541064 172689 net.cpp:141] Setting up pool3
I0412 21:14:09.541087 172689 net.cpp:148] Top shape: 100 64 4 4 (102400)
I0412 21:14:09.541101 172689 net.cpp:156] Memory required for data: 31950000
I0412 21:14:09.541117 172689 layer_factory.hpp:77] Creating layer ip1
I0412 21:14:09.541136 172689 net.cpp:91] Creating Layer ip1
I0412 21:14:09.541152 172689 net.cpp:425] ip1 <- pool3
I0412 21:14:09.541172 172689 net.cpp:399] ip1 -> ip1
I0412 21:14:09.545353 172689 net.cpp:141] Setting up ip1
I0412 21:14:09.545387 172689 net.cpp:148] Top shape: 100 64 (6400)
I0412 21:14:09.545418 172689 net.cpp:156] Memory required for data: 31975600
I0412 21:14:09.545449 172689 layer_factory.hpp:77] Creating layer ip2
I0412 21:14:09.545508 172689 net.cpp:91] Creating Layer ip2
I0412 21:14:09.545524 172689 net.cpp:425] ip2 <- ip1
I0412 21:14:09.545549 172689 net.cpp:399] ip2 -> ip2
I0412 21:14:09.545730 172689 net.cpp:141] Setting up ip2
I0412 21:14:09.545750 172689 net.cpp:148] Top shape: 100 10 (1000)
I0412 21:14:09.545768 172689 net.cpp:156] Memory required for data: 31979600
I0412 21:14:09.545792 172689 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0412 21:14:09.545809 172689 net.cpp:91] Creating Layer ip2_ip2_0_split
I0412 21:14:09.545822 172689 net.cpp:425] ip2_ip2_0_split <- ip2
I0412 21:14:09.545836 172689 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0412 21:14:09.545855 172689 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0412 21:14:09.545910 172689 net.cpp:141] Setting up ip2_ip2_0_split
I0412 21:14:09.545930 172689 net.cpp:148] Top shape: 100 10 (1000)
I0412 21:14:09.545946 172689 net.cpp:148] Top shape: 100 10 (1000)
I0412 21:14:09.545960 172689 net.cpp:156] Memory required for data: 31987600
I0412 21:14:09.545975 172689 layer_factory.hpp:77] Creating layer accuracy
I0412 21:14:09.545995 172689 net.cpp:91] Creating Layer accuracy
I0412 21:14:09.546010 172689 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0412 21:14:09.546025 172689 net.cpp:425] accuracy <- label_cifar_1_split_0
I0412 21:14:09.546039 172689 net.cpp:399] accuracy -> accuracy
I0412 21:14:09.546058 172689 net.cpp:141] Setting up accuracy
I0412 21:14:09.546074 172689 net.cpp:148] Top shape: (1)
I0412 21:14:09.546087 172689 net.cpp:156] Memory required for data: 31987604
I0412 21:14:09.546102 172689 layer_factory.hpp:77] Creating layer loss
I0412 21:14:09.546118 172689 net.cpp:91] Creating Layer loss
I0412 21:14:09.546133 172689 net.cpp:425] loss <- ip2_ip2_0_split_1
I0412 21:14:09.546146 172689 net.cpp:425] loss <- label_cifar_1_split_1
I0412 21:14:09.546164 172689 net.cpp:399] loss -> loss
I0412 21:14:09.546185 172689 layer_factory.hpp:77] Creating layer loss
I0412 21:14:09.546536 172689 net.cpp:141] Setting up loss
I0412 21:14:09.546557 172689 net.cpp:148] Top shape: (1)
I0412 21:14:09.546569 172689 net.cpp:151]     with loss weight 1
I0412 21:14:09.546591 172689 net.cpp:156] Memory required for data: 31987608
I0412 21:14:09.546608 172689 net.cpp:217] loss needs backward computation.
I0412 21:14:09.546623 172689 net.cpp:219] accuracy does not need backward computation.
I0412 21:14:09.546638 172689 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0412 21:14:09.546653 172689 net.cpp:217] ip2 needs backward computation.
I0412 21:14:09.546664 172689 net.cpp:217] ip1 needs backward computation.
I0412 21:14:09.546679 172689 net.cpp:217] pool3 needs backward computation.
I0412 21:14:09.546694 172689 net.cpp:217] relu3 needs backward computation.
I0412 21:14:09.546707 172689 net.cpp:217] conv3 needs backward computation.
I0412 21:14:09.546720 172689 net.cpp:217] pool2 needs backward computation.
I0412 21:14:09.546736 172689 net.cpp:217] relu2 needs backward computation.
I0412 21:14:09.546748 172689 net.cpp:217] conv2 needs backward computation.
I0412 21:14:09.546762 172689 net.cpp:217] relu1 needs backward computation.
I0412 21:14:09.546774 172689 net.cpp:217] pool1 needs backward computation.
I0412 21:14:09.546789 172689 net.cpp:217] conv1 needs backward computation.
I0412 21:14:09.546804 172689 net.cpp:219] label_cifar_1_split does not need backward computation.
I0412 21:14:09.546818 172689 net.cpp:219] cifar does not need backward computation.
I0412 21:14:09.546833 172689 net.cpp:261] This network produces output accuracy
I0412 21:14:09.546846 172689 net.cpp:261] This network produces output loss
I0412 21:14:09.546875 172689 net.cpp:274] Network initialization done.
I0412 21:14:09.546960 172689 solver.cpp:60] Solver scaffolding done.
I0412 21:14:09.547469 172689 caffe.cpp:209] Resuming from examples/cifar10/cifar10_quick_iter_4000.solverstate.h5
I0412 21:14:09.550253 172689 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0412 21:14:09.553784 172689 caffe.cpp:219] Starting Optimization
I0412 21:14:09.553824 172689 solver.cpp:279] Solving CIFAR10_quick
I0412 21:14:09.553889 172689 solver.cpp:280] Learning Rate Policy: fixed
I0412 21:14:09.554684 172689 solver.cpp:337] Iteration 4000, Testing net (#0)
I0412 21:14:09.593173 172689 blocking_queue.cpp:50] Data layer prefetch queue empty
I0412 21:14:09.937292 172689 solver.cpp:404]     Test net output #0: accuracy = 0.7167
I0412 21:14:09.937353 172689 solver.cpp:404]     Test net output #1: loss = 0.833817 (* 1 = 0.833817 loss)
I0412 21:14:09.943347 172689 solver.cpp:228] Iteration 4000, loss = 0.590933
I0412 21:14:09.943385 172689 solver.cpp:244]     Train net output #0: loss = 0.590933 (* 1 = 0.590933 loss)
I0412 21:14:09.943404 172689 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0412 21:14:10.847753 172689 solver.cpp:228] Iteration 4100, loss = 0.632354
I0412 21:14:10.847801 172689 solver.cpp:244]     Train net output #0: loss = 0.632354 (* 1 = 0.632354 loss)
I0412 21:14:10.847815 172689 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0412 21:14:11.750303 172689 solver.cpp:228] Iteration 4200, loss = 0.56332
I0412 21:14:11.750365 172689 solver.cpp:244]     Train net output #0: loss = 0.56332 (* 1 = 0.56332 loss)
I0412 21:14:11.750383 172689 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0412 21:14:12.652122 172689 solver.cpp:228] Iteration 4300, loss = 0.460877
I0412 21:14:12.652205 172689 solver.cpp:244]     Train net output #0: loss = 0.460877 (* 1 = 0.460877 loss)
I0412 21:14:12.652228 172689 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0412 21:14:13.557417 172689 solver.cpp:228] Iteration 4400, loss = 0.508737
I0412 21:14:13.557488 172689 solver.cpp:244]     Train net output #0: loss = 0.508737 (* 1 = 0.508737 loss)
I0412 21:14:13.557509 172689 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0412 21:14:14.451633 172689 solver.cpp:337] Iteration 4500, Testing net (#0)
I0412 21:14:14.811851 172689 solver.cpp:404]     Test net output #0: accuracy = 0.7531
I0412 21:14:14.811915 172689 solver.cpp:404]     Test net output #1: loss = 0.738247 (* 1 = 0.738247 loss)
I0412 21:14:14.816076 172689 solver.cpp:228] Iteration 4500, loss = 0.52857
I0412 21:14:14.816125 172689 solver.cpp:244]     Train net output #0: loss = 0.52857 (* 1 = 0.52857 loss)
I0412 21:14:14.816141 172689 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0412 21:14:15.720854 172689 solver.cpp:228] Iteration 4600, loss = 0.547537
I0412 21:14:15.720919 172689 solver.cpp:244]     Train net output #0: loss = 0.547537 (* 1 = 0.547537 loss)
I0412 21:14:15.720935 172689 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0412 21:14:16.628778 172689 solver.cpp:228] Iteration 4700, loss = 0.516757
I0412 21:14:16.628841 172689 solver.cpp:244]     Train net output #0: loss = 0.516757 (* 1 = 0.516757 loss)
I0412 21:14:16.628859 172689 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0412 21:14:17.533713 172689 solver.cpp:228] Iteration 4800, loss = 0.430438
I0412 21:14:17.533917 172689 solver.cpp:244]     Train net output #0: loss = 0.430438 (* 1 = 0.430438 loss)
I0412 21:14:17.534013 172689 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0412 21:14:18.436372 172689 solver.cpp:228] Iteration 4900, loss = 0.496829
I0412 21:14:18.436434 172689 solver.cpp:244]     Train net output #0: loss = 0.496829 (* 1 = 0.496829 loss)
I0412 21:14:18.436451 172689 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0412 21:14:19.325289 172689 solver.cpp:464] Snapshotting to HDF5 file examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5
I0412 21:14:19.332340 172689 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_quick_iter_5000.solverstate.h5
I0412 21:14:19.338711 172689 solver.cpp:317] Iteration 5000, loss = 0.507877
I0412 21:14:19.338762 172689 solver.cpp:337] Iteration 5000, Testing net (#0)
I0412 21:14:19.693274 172689 solver.cpp:404]     Test net output #0: accuracy = 0.7561
I0412 21:14:19.693328 172689 solver.cpp:404]     Test net output #1: loss = 0.733123 (* 1 = 0.733123 loss)
I0412 21:14:19.693343 172689 solver.cpp:322] Optimization Done.
I0412 21:14:19.693359 172689 caffe.cpp:222] Optimization Done.
